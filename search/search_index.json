{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the SFU Cedar documentation! To edit these pages please make a pull request Other sources of documentations: ATLAS Canada twiki Compute Canada twiki","title":"Home"},{"location":"atlas/","text":"First Time Configurations To set up your environment for ATLAS-specific work, add the following lines to the file $HOME/.bashrc export RUCIO_ACCOUNT=<your lxplus username> source /home/atlas/Tier3/AtlasUserSiteSetup.sh Thus will set up the setupATLAS alias and allow you to use rucio . If you plan to use the grid, install your grid certificates in your $HOME/.globus directory following the instructions here . Containers Unlike lxplus, Cedar is not set up to natively run ATLAS software. To set up an lxplus-like environment, it is necessary to run in a containerized environment. This is done by running setupATLAS -c centos7+batch from your home directory. This will put you in a singularity container that will give you an environment similar to that of lxplus. You can then run all ATLAS software as you normally would. Submitting batch jobs You can submit and access the batch queues from inside your interactive container session. For example, if you type setupATLAS -c centos7+batch you can next generate a script to submit to the batch system. eg. batchScript \"source <somepath in container>/myJob.sh\" -o submitMyJob.sh (The above will create a file submitMyJob.sh that will run myJob.sh inside the same type of container you are on now but on the batch queue. Storage Cedar is co-located with the Tier2 site 'CA-SFU-T2', which allows you to access Tier2 storage directly from the Tier3. Requesting Rucio rules Each user has a 5Tb quota on the CA-SFU-T2_LOCALGROUPDISK grid site. To request replication of a dataset to this site, see here (note: requires installing your grid certificate in your browser). Finding the local files A script is provided to determine the file paths using the rucio file checksum. Usage: python discoverLocalPaths.py dsidList1.txt ... dsidListN.txt where dsidListX.txt is a text file containing a list of rucio datasets that you which to locate. For each dsidListX.txt , this script will output a file pathLists_dsidListX.txt which contains the local paths to all of the files in the input datasets.","title":"ATLAS setup"},{"location":"atlas/#first-time-configurations","text":"To set up your environment for ATLAS-specific work, add the following lines to the file $HOME/.bashrc export RUCIO_ACCOUNT=<your lxplus username> source /home/atlas/Tier3/AtlasUserSiteSetup.sh Thus will set up the setupATLAS alias and allow you to use rucio . If you plan to use the grid, install your grid certificates in your $HOME/.globus directory following the instructions here .","title":"First Time Configurations"},{"location":"atlas/#containers","text":"Unlike lxplus, Cedar is not set up to natively run ATLAS software. To set up an lxplus-like environment, it is necessary to run in a containerized environment. This is done by running setupATLAS -c centos7+batch from your home directory. This will put you in a singularity container that will give you an environment similar to that of lxplus. You can then run all ATLAS software as you normally would.","title":"Containers"},{"location":"atlas/#submitting-batch-jobs","text":"You can submit and access the batch queues from inside your interactive container session. For example, if you type setupATLAS -c centos7+batch you can next generate a script to submit to the batch system. eg. batchScript \"source <somepath in container>/myJob.sh\" -o submitMyJob.sh (The above will create a file submitMyJob.sh that will run myJob.sh inside the same type of container you are on now but on the batch queue.","title":"Submitting batch jobs"},{"location":"atlas/#storage","text":"Cedar is co-located with the Tier2 site 'CA-SFU-T2', which allows you to access Tier2 storage directly from the Tier3.","title":"Storage"},{"location":"atlas/#requesting-rucio-rules","text":"Each user has a 5Tb quota on the CA-SFU-T2_LOCALGROUPDISK grid site. To request replication of a dataset to this site, see here (note: requires installing your grid certificate in your browser).","title":"Requesting Rucio rules"},{"location":"atlas/#finding-the-local-files","text":"A script is provided to determine the file paths using the rucio file checksum. Usage: python discoverLocalPaths.py dsidList1.txt ... dsidListN.txt where dsidListX.txt is a text file containing a list of rucio datasets that you which to locate. For each dsidListX.txt , this script will output a file pathLists_dsidListX.txt which contains the local paths to all of the files in the input datasets.","title":"Finding the local files"},{"location":"cedar/","text":"Logging in to Cedar To ssh into the Cedar login node, run ssh -Y [username]@cedar.computecanada.ca where [username] is your SFU computing ID. Directories Cedar has several storage spaces or filesystems and you should ensure that you are using the right space for the right task. HOME: While your home directory may seem like the logical place to store all your files and do all your work, in general this isn't the case - your home normally has a relatively small quota and doesn't have especially good performance for the writing and reading of large amounts of data. The most logical use of your home directory is typically source code, small parameter files and job submission scripts. PROJECT: The project space has a significantly larger quota and is well-adapted to sharing data among members of a research group since it, unlike the home or scratch, is linked to a professor's account rather than an individual user. The data stored in the project space should be fairly static, that is to say the data are not likely to be changed many times in a month. Otherwise, frequently changing data - including just moving and renaming directories - in project can become a heavy burden on the tape-based backup system. SCRATCH: For intensive read/write operations on large files (> 100 MB per file), scratch is the best choice. Remember however that important files must be copied off scratch since they are not backed up there, and older files are subject to purging. The scratch storage should therefore be used for temporary files: checkpoint files, output from jobs and other data that can easily be recreated. See here and here for more details. Resource intensive work The interactive login nodes (and containers started from them) are meant for light work such as compiling and running a quick test job and for submitting to the batch system. Performing resource intensive tasks on these nodes can cause issues for other users and is forbidden. All resource intensive tasks must use the batch system. Submitting batch jobs Cedar uses the SLURM job scheduler to manage batch jobs. See here for a guide. Interactive work If you need to do resource-intensive interactive work, you will need to get a resource allocation using the schedule. To do this, you can run (for example) salloc --time=16:0:0 --mem-per-cpu=32G --cpus-per-task=1 --account=ctb-stelzer --nodelist=cdr1642 This will request a single CPU for 16 hours, with 32Gb of memory. You may need to replace ctb-stelzer with the name of the user that sponsored your account. Disk quota exceeded troubleshooting On cedar, files that you create will automatically be associated to your primary group , such as ctb-stelzer . This is relevant because the disk quotas are managed on a per-group basis. However, certain applications (such as MadGraph) can create files in a different group, which can lead to disk quota exceeded issues. To fix this, you can wrap the problematic command with sg def-groupname -c . E.g. sg def-stelzer -c \"./bin/mg5_aMC myScript.mg5\"","title":"Cedar basics"},{"location":"cedar/#logging-in-to-cedar","text":"To ssh into the Cedar login node, run ssh -Y [username]@cedar.computecanada.ca where [username] is your SFU computing ID.","title":"Logging in to Cedar"},{"location":"cedar/#directories","text":"Cedar has several storage spaces or filesystems and you should ensure that you are using the right space for the right task. HOME: While your home directory may seem like the logical place to store all your files and do all your work, in general this isn't the case - your home normally has a relatively small quota and doesn't have especially good performance for the writing and reading of large amounts of data. The most logical use of your home directory is typically source code, small parameter files and job submission scripts. PROJECT: The project space has a significantly larger quota and is well-adapted to sharing data among members of a research group since it, unlike the home or scratch, is linked to a professor's account rather than an individual user. The data stored in the project space should be fairly static, that is to say the data are not likely to be changed many times in a month. Otherwise, frequently changing data - including just moving and renaming directories - in project can become a heavy burden on the tape-based backup system. SCRATCH: For intensive read/write operations on large files (> 100 MB per file), scratch is the best choice. Remember however that important files must be copied off scratch since they are not backed up there, and older files are subject to purging. The scratch storage should therefore be used for temporary files: checkpoint files, output from jobs and other data that can easily be recreated. See here and here for more details.","title":"Directories"},{"location":"cedar/#resource-intensive-work","text":"The interactive login nodes (and containers started from them) are meant for light work such as compiling and running a quick test job and for submitting to the batch system. Performing resource intensive tasks on these nodes can cause issues for other users and is forbidden. All resource intensive tasks must use the batch system.","title":"Resource intensive work"},{"location":"cedar/#submitting-batch-jobs","text":"Cedar uses the SLURM job scheduler to manage batch jobs. See here for a guide.","title":"Submitting batch jobs"},{"location":"cedar/#interactive-work","text":"If you need to do resource-intensive interactive work, you will need to get a resource allocation using the schedule. To do this, you can run (for example) salloc --time=16:0:0 --mem-per-cpu=32G --cpus-per-task=1 --account=ctb-stelzer --nodelist=cdr1642 This will request a single CPU for 16 hours, with 32Gb of memory. You may need to replace ctb-stelzer with the name of the user that sponsored your account.","title":"Interactive work"},{"location":"cedar/#disk-quota-exceeded-troubleshooting","text":"On cedar, files that you create will automatically be associated to your primary group , such as ctb-stelzer . This is relevant because the disk quotas are managed on a per-group basis. However, certain applications (such as MadGraph) can create files in a different group, which can lead to disk quota exceeded issues. To fix this, you can wrap the problematic command with sg def-groupname -c . E.g. sg def-stelzer -c \"./bin/mg5_aMC myScript.mg5\"","title":"Disk quota exceeded troubleshooting"}]}